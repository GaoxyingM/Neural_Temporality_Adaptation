[rnn]
# max number of sequence
seq_max_len = 60
# number of cells for RNN
rnn_size = 100
l1_rate = 0.0
l2_rate = 0.0001
# activation function
rnn_ac = tanh
# dropout rate
dp_rate = 0.2
dense_size = 100
# activation function for the dense layer
dense_ac = relu
# number of classes, if binary, then 1.
num_cl = 1
# loss function
loss = binary_crossentropy
# optimizer
opt = rmsprop
epochs = 20
# specify the weights for each label catgory
# eg, {1:3, 0: 1} or auto
class_wt = auto
batch_size = 64
lr = 0.001


[cnn]
# max number of sequence
seq_max_len = 60
kernel_size = 4
pool_size=2
padding=valid
filters = 200
strides=1
l1_rate = 0.001
l2_rate = 0.001
# dropout rate
dp_rate = 0.1
dense_size = 100
# activation function for the dense layer
dense_ac = relu
# number of classes, if binary, then 1.
num_cl = 1
# loss function
loss = binary_crossentropy
# optimizer, rmsprop or adam
opt = rmsprop
epochs = 20
# specify the weights for each label catgory
# eg, {1:3, 0: 1} or auto
class_wt = auto
batch_size = 64
lr = 0.0001


[cnn_rnn]
# number of cells for RNN
rnn_size = 100
rnn_ac = tanh
# max number of sequence
seq_max_len = 60
kernel_size = 3
padding=valid
filters = 150
strides=1
pool_size = 3
l1_rate = 0.0
l2_rate = 0.0001
# dropout rate
dp_rate = 0.1
dense_size = 100
# activation function for the dense layer
dense_ac = relu
# number of classes, if binary, then 1.
num_cl = 1
# loss function
loss = binary_crossentropy
# optimizer
opt = rmsprop
epochs = 20
# specify the weights for each label catgory
# eg, {1:3, 0: 1} or auto
class_wt = auto
batch_size = 64
lr = 0.001


[data]
data_dir = ./split_data_idx/amazon_year
# embedding weight dir
wt_dir = ./domain_weights/amazon_year
